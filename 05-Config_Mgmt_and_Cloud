Some Definitions:

Scale           Being able to scale what we do means that we can keep achieving larger impacts with the same amount of effort
                A scalable system is a flexible one!

                To find out if a system is scalable, you could ask following questions:
                - Will adding more servers increase the capacity of the service?
                - How are new servers prepared, installed, and configured? 
                - How quickly can you set up new computers to get them ready to be used? 
                - Could you deploy a hundred servers with the same IT team that you have today? 
                - Or would you need to hire more people to get it done faster? 
                - Would all the deployed servers be configured exactly the same way?

Automation      An essential tool for keeping up with the infrastructure needs of a growing business.
   |            The process of replacing a manual step with one that happens automatically.
   |
    > Automation is when we set up a single step in a process to require no oversight, 
    > while orchestration refers to automating the entire process.
   |
Orchestration   The automated configuration and coordination of complex IT systems and services

Configuration Management (some Tools: Puppet, Chef, Ansible, CFEngine)
                - define configuration rules
                - let automation apply those rules on all machines (also called "nodes")

                -> Another important aspect of configuration management is that operations should be idempotent. 
                   In this context, an idempotent action can be performed over and over again without changing the system after the first time 
                   the action was performed, and with no unintended side effects.

                - Test and repair paradigm: first test if an action is needed on client and - only then - perform action (repair)
                - Puppet is stateless: Each Puppet run is independent of the previous one, and the next one. Every run just depends on currently colledted facts.

Infrastructure as Code 
                When all of the configuration necessary to deploy and manage a node in the infrastructure is stored in version control.
                Managing your Infrastructure as Code means that your fleet of nodes are 
                    - consistent,
                    - versioned, 
                    - reliable and 
                    - repeatable. 
                Instead of being seen as precious or unique, machines are treated as replaceable resources that can be deployed on-demand through the automation. 
                Any infrastructure that claims to be scalable must be able to handle the capacity requirements of growth.

====================================================================================================================================================================================
Puppet

Managing the configurations of computers and a fleet of machines.

          |-> process---|
          |  Manifests  |
          |            \|/
        Puppet master (service)
           ^            |
    sends  |            |   generates Catalog
    Facts  |            |
           |           \|/
        Puppet agent (client)
 (uses "Providers" to apply rules to local system)


 Puppet Agent       every physical machine, VM, Server, Routers, ... that have a puppet agent installed and can apply the rules

Facts               Variables that represent characteristics of the system
Providers           Puppet assigns providers according to predefined rules for the resource type and data collected from the system such as the family of the underlying operating system.
                    Providers may be package managers for installation of software, e.g. apt, yum

Resources           The basic unit for modeling that we want to manage (e.g. 'package' or 'file' keyword is a resource)
                    The most basic unit in Puppet is a resource, such as user, group, file, service or package.

Puppet Syntax:      class class_name {
                        resource { 'title_of_resource': 
                            attribute1 => set_value1,
                            attribute2 => set_value2,
                            ...
                        }
                    }

Puppet uses a declarative language because we declare the state that we want to achieve rather than the steps to get there.
(vs. Python, etc. being procedural languages.)

Classes:            group related resources
                    more easily understand the configuration and make changes in the future

Resources:
    exec                     # runs a specific command                  <-- be careful as this resource might not be idempotent
    file
    package
    service


Attributes ==> values:
    content => some_value               # content to be in e.g. a file
    ensure => absent                    # a resource will be removed
    command => command                  # command to be executed in an exec resource, e.g. 'mv file1 Desktop/file1', combine with attribute "onlyif" to make it idempotent
    ensure => directory                 # this resource must be available on the client and being a directory
    ensure => file                      # this resource must be available on the client and being a file
    ensure => installed
    ensure => latest                    # install latest version
    ensure => purged
    ensure => running                   # start a service if not already running
    mode => '0644'                      # set special rights, rights for owner, group, others (rwx)
    onlyif => command                   # performs command only if it is possible  <-- this is idempotent and will not finish with an error
    replace => true                     # an existing file will be overwritten


Puppet can provide, install and run a service:
class ntp {                     # ntp: Network Time Protocol: method computers uses to synchronize clocks
    package {'ntp':
        ensure => latest,
    }
    file {'etc/ntp.conf':
        source => 'link',
        replace => true,
    }
    service {'ntp':
        enable: true,
        ensure => running,
    }
}

------------------------------------------------------------------------------------------------------------------------------------------------
Puppet - Domain Specific Language (DSL)

Python, Java, Ruby, Go, .... are General purpose languages
vs.
Domain Specific Languages       A programming language that is more limited in scope

DSL for Puppet defines
    - when
    - how 
      to apply configuration management rules to our devices

    - Variables                 always written as $var
    - Conditional Statements
    - Functions

    - Facts                     Variables that represent characteristics of the system 
                                adress facts in DSL with $facts['fact_name']
                                facts are hashes (equivalent to dictionaries in Python)
                                A fact is a hash that stores information about the details of a particular system.

Facts:
    $fact['is_virtual']         # checks if the system is a virtual machine


------------------------------------------------------------------------------------------------------------------------------------------------
local usage of puppet

Installation:       sudo apt install puppet-master
Puppet Files:       called "Manifest", always ends on .pp
Manifest:           a file in Puppet
Module:             a collection of manifests and associated data always starts with a file init.pp
                        and it should define a class with the same name as the module that you're creating
Template:           files that are preprocessed before they've been copied into the client machines

bash:> vim tools.pp

package {'htop':            # this package ensures that the htop command is present on the computer
    ensure => present,
}

bash:> sudo puppet apply -v tools.pp           # -v: verbose output

Catalog                The list of rules that are generated for one specific computer once the server has evaluated 
                            all variables, conditionals, and functions

Resource Relationship:
We write resource types in lowercase when declaring them, but capitalize them when referring to them from another resource's attributes.

class ntp {
    package {'ntp':
        ensure => latest,
    }
    file {'/etc/ntp.conf':
        ...
        require => Package['ntp'],             # requires previoulsy defined package
        notify => Service['ntp],               # all relationships like Package and Service are written with a capital first letter
                                               #   while the relevant resources (package, file, service) are written with a first lower letter
                                               # When defining resource types, we write them in lowercase, then capitalize them when referring 
                                               #   to them from another resource attribute.
    }
    service {'ntp':
        ...
        require => File['/etc/ntp.conf'],      # requires previoulsy defined file
        ...
    }
}

include ntp                                    # tell puppet that the rules of the class shall be applied

------------------------------------------------------------------------------------------------------------------------------------------------
modules                         # A module is a collection of resources, and associated data used to expand the functionality of Puppet.
|--module-name
    |--files                    #  files folder in a module will contain files that won’t need to be changed like configuration files.  
    |   |--module-name.comf
    |
    |--lib                      # new functions added after installing a new module can be found in the lib folder in the directory of the new module.
    |   |--adds_functions_and_facts_already_shipped_by_puppet
    |
    |--manifests                
    |   |--init.pp              # contains a class with a name of the module-name
    |
    |--templates                # Templates are documents that combine code, system facts, and text to render a configuration output fitting predefined rules.  
    |   |                       # Templates are documents that combine code, data, and literal text to produce a final rendered output. The goal of a template 
    |   |                       #       is to manage a complicated piece of text with simple inputs
    |   |--templates.file
    |
    |--metadata.json            #  includes some additional data about the module we just installed, like which versions of which operating systems it's compatible with

Templates
Templates are written in a templating language, which is specialized for generating text from data. Puppet supports two templating languages:
Embedded Puppet (EPP) uses Puppet expressions in special tags. It's easy for any Puppet user to read, but only works with newer Puppet versions. (≥ 4.0, or late 3.x versions with future parser enabled.)
Embedded Ruby (ERB) uses Ruby code in tags. You need to know a small bit of Ruby to read it, but it works with all Puppet versions.

# there are predefined modules in puppet
# install them like e.g. sudo apt install puppet-module-puppetlabs-apache
# module files are stored under: /usr/share/puppet/modules.available/puppetlabs-apache
# use predefined modules by creating own module, e.g. webserver.pp with only one line

        include ::apache                       # double colon before name shows puppet that this is a GLOBAL module

# then apply with: sudo puppet apply -v webserver.pp

------------------------------------------------------------------------------------------------------------------------------------------------
Deploying Puppets to Clients

Node            Any System where we can run a Puppet agent 


Deploy different puppet rules on different machines by:
    - conditionally apply rules based on facts from the machines
    - Node definitions:

                node default {
                    class { 'sudo':  }                                                      # include class with no additional settings
                    class { 'ntp':                                                          # include class and define additional parameters
                        servers => ['ntp1.example.com','ntp2.example.com'] }
                }
                node webserver.example.com {                                                # specific node is defined by FQDN: fully qualified domain name
                                                                                            # this node will only be applied to machines that match the FQDN
                    class { 'sudo':  }      # include class with no additional settings
                    class { 'ntp':          # include class and define additional parameters
                        servers => ['ntp1.example.com','ntp2.example.com'] 
                    }
                    class { 'apached': }
                }

    Node definitions are usually stored in a file called "/site.pp" (<- it is not part of any module)

# How is ensured that puppet receives valid information about a computer name (security issue!!!)
--> Puppet uses PKI (Publik Key Infrastructure) / SSL (Secure Sockets Layer)
        each machine has a    -> private key               -> public key

        Certificate Authority (CA) verifies the identities of the machines --> create Certificate that states the public key goes with the machine
        take the CA from Puppet or an external one

        Certifocate Authority:  The CA either queues a certificate request for manual validation, or uses pre-shared data to verify before sending 
                                    the certificate to the agent.

        - CA becomes active when a node checks in to the puppet master for the first time
        - CA requests a certificate
        - Puppet master creates Certificate for Node if identity can be verified (by System's admin or an automatic process)
        - now the Node has a valid certificate which it can use to identify itself when requesting a catalogue

--> why is security so important
        - Puppet Rules may contain confidential content
        - being sure to only use correct machines in network (no rogue machines)

In Test Environment Puppet can be configured to automatically sign every request (but NEVER do this in a productive environement)
--> sudo puppet config --section master set autosign true

BETTER BE SAFE THAN SORRY | vs. | Besser um Verzeichung bitten als um Erlaubnis (Charly Harper).

---------------------------------------------------------------------------------------------------------------------------------------
Setting up Puppet Client and Masters

on Puppet Master (ubuntu.example.com):
    TEST ONLY:  sudo puppet config --section master set autosign true
    ssh webserver.example.com

on Puppet Agent (webserver.example.com):
    sudo apt install puppet                                 # install Puppet Agent
    sudo puppet config set server ubuntu.example.com        # set Puppet Master
    sudo puppet agent -v --test                             # test connection to Puppet Master

on Puppet Master (ubuntu.example.com):
    create site.pp

    node webserver.example.com {
        class {'apache':}
    }
    node default {}

on Puppet Agent (webserver.example.com):
    sudo puppet agent -v --test                             # Test - OK?
    
    # systemctl process which lets us enable the services actually running
    sudo systemctl enable puppet                            # start puppet every time the system is rebooted so puppet agent pulls updates to config files automatically
    sudo systemctl start puppet                             # now, manually start ctl process
    sudo systemctl status puppet                            # see actual status, is ctl started?

Production              the parts of the infrastructure where a service is executed and served to its users

machine_info module     gathers some information from the machine using Puppet facts and then stores it in a file.


# Rolling out new / changed Puppet Scripts to Production:
    - Test Environment
        - check syntax of script with "puppet purser validate" command
        - run the rules using the "no op"-parameter (the name comes from no operations and it makes puppet simulate what it would do without actually doing it. You can look at 
                                                        the list of actions that it would take and check that they're exactly what you wanted puppet to do)
        - apply rules on test machines (Puppet rspec Tests: We can set the facts involved different values and check that the catalog ends up stating what we wanted it to)

    - Baked In Environments in Puppet
    
    - push to PROD in batches / canaries
        - use fact "early adopters"


---------------------------------------------------------------------------------------------------------------------------------------
The Cloud

Advantages of using cloud service:
    Cloud services provide many advantages, like
    - outsourcing support and maintenance, simplifying configuration management, and letting the provider take care of security.
    - simplifying configuration management, outsourcing support and maintenance, and letting the provider take care of security.
    - putting the provider in charge of security.


Software as a Service (SaaS)            When a Cloud provider delivers an entire application or program to the customer, e.g. Gmail, Dropbox, MS365
                                        Cloud Provider has full control of how the application runs
Platform as a Service (PaaS)            When a Cloud provider offers a preconfigured platform to the customer, e.g. SQL database as a Service, Managed Web Apps
                                                (famous Managed Web Apps: Amazon Elastic Beanstalk, Microsoft App Service, Google App Engine)
                                        We are in charge of the Code, but we are not responsible for running the application
Infrastructure as a Service (IaaS)      When a Cloud provider supplies only the bare-bones computing experience, e.g. run VM in the Cloud
                                        We decide the OS of the machine, the software that is running, and so on ...
                                        Very helpful for "Lift and Shift" Strategy:  When we migrate from traditional server configurations to the Cloud, 
                                                        we lift the current configuration and shift it to a virtual machine

Public Cloud                            The cloud service provided by to you by a third party (services are offered to the public)
Private Cloud                           When your company owns the services and the rest of the infrastructure, whether that's on-site or in a remote data center
Hybrind Cloud                           A mixture of both public and private clouds (it is important that services are integrated seamingless)
Multi-Cloud                             A mixture of public and/or private clouds across vendors (gives extra protection but may be more expensive)

Containers                              Applications that are packaged together with their configuration and dependencies
                                        Packaged applications that are shipped together with their libraries and dependencies.
                                        A virtualized environment containing applications and configurations that can run quickly and reliably on any computing environment.
                                        If application runs in a container it can run in a container everywhere!


DataCenters are located in REGIONS and in ZONES; choose a DataCenter next to you due to:
    - latency
    - eventually law policy reasons
    - place depending Servers next to each other


Scaling in the Cloud

Capacity                How much the service can deliver

Scaling can be done
    - horizontally      add more node to a specific service, more servers
    - vertically        make nodes bigger (increase capacity, e.g. memory, cpu, disk size)

    - automatically     the service offered by the Cloud provider will use metrics to automatically increase or decrease the capaxity of the system
                        (send right quotas for autoscaling system)
    - manually          changes are made by humans instead of software (less complicated, but needs monitoring and might not scale quick enough)


Important for Selection of Cloud Provider
    - level of support (in case of any problems)
    - Security measures cloud provider has taken (certifications like SOC 1, ISO 27001, ...)

        ! We should always use reasonable judgment to protect the machines that we deploy, whether that's on physical servers running on-premise or on virtual machines in the Cloud !

-----------------------------------------------------------------------------------------------------------------------------------------------
Parameters to be set for a new Google Cloud Instance
    - Name of the Instance
    - Region where the Instance is running
    - Zone in Region where the Instance is running
    - Machine Type (virtual CPUs, Memory)
    - Boot Disk (contains OS and additional Disk Space)
    - Operating System (OS)

Reference Machine               Store the contents of a machine in a reusable format (=> Disk Image).
Templating                      Process of capturing all of the system configuration to let us create VMs in a repeatable way
Disk Image                      A snapshop of a virtual machine's disk at a given point in time.
Reference Image                 File or Configuration that can be deployed repeatedly with automatated tools
systemd-File                    Initializing system used by most modern Linux distributions
systemd-Directory               /etc/systemd/system/ is the default systemd directory in many Linux distros, including Red Hat Linux.
                                /etc/systemd/system/ -> copy service files on Linux in order to configure it as services
                                start with: sudo systemctl enable <script_name>
Load Balancer                   Ensures that each node receives a balanced number of requests
        -> Round Robin          give each Node one Request (simplest form of Load Balancing)

Autoscaling                     Allows the service to increase or reduce capacity as needed while the service owner only pays for the cost 
                                        of the machines that are in use at any given time.


Google Cloud - console.cloud.google.com
    - Create a project
        - Give it a project name
    - Compute Engine > VM Instances
        - Create
        - Name, Region, Zone
        - Type of Machine: General-Purpose / Memory-Optimized
        - CPU, Memory
        - Disk (Standard Disk / SSD, Size)
        - Configure how to access the Cloud (SSH, RDP, ...)
        - Firewall Options
        - more options ...
        - click on "command line" link and see how VM is created via the command line
        - Create!

Templating a Customized VM
    - Shut Down VM
    - Klick on VM's name for further details
    - Click on Boot Disk
    - Create Image
    - Give it a name
    - Check Source disk
    - Create

    - Then go to Instance templates
    - Create Instance Template
    - Change name
    - Change boot disk to above generated Image
    - Select "Allow HTTP traffic"
    - Create

    - Go to VM instances
    - Create instance
    - New VM from template
    - Select Template
    - Check that HTTP traffic is allowed
    - Create

Templating Several Customized VM
- Do it via Console - gCloud command
        gcloud init                             # The gcloud init command sets up the authentication procedure between our virtual machine and Google Cloud.
                        Authenticate!
                        Select default project
        gcloud compute instances create  --source-instance-template webserver-template ws1 ws2 ws3 ws4 ws5
                        # compute           Parameter, that is used for everything that has to do with VMs
                        # instances         Parameter, when dealing with the VM instances themselves
                        # create            Parameter to create instances 
                        # --source-instance-template        A template shall be used for creation of the VMs
                        # webserver-template                Name of the template
                        # ws1 - ws5                         Name of the VMs to be created / Name of the instances that shall be deployed


Deeper look at a bigger Web Service
- Browser connects to site through the internet
- Browser receives IP address of the website (IP address identifies a specific computer, the entry point for the sites)
  Commonly there will be a bunch of different entry points for a single website. This allows the service to stay up even if one of them fails. 
  On top of that, it's possible to select an entry point that's closer to the user to reduce latency. In a small-scale application, this entry point 
  could be the web server that serves the pages, and that would be it. For large applications where speed and availability matter, there will be a 
  couple of layers in between the entry point and the actual web service.

- The first layer will be a pool of web caching servers with a load balancer to distribute the requests among them (e.g. Varnish, Nginx, Cloudflare, Fastly) 
- When a request is made, the caching servers first check if the content is already stored in their memory. 
    - If it's there, they respond with the contents, 
    - if it's not, they ask their configured backend for the content and then store it so that it's present for future requests. This configured backend is 
      the actual web service that generates the webpages for the site, and it will also normally be a pool of nodes running under a load balancer. To get any 
      necessary data, this service will connect to a database. But because getting data from a database can be slow, there's usually an extra layer of caching, 
      specific for the database contents. The most popular applications for this level of caching are Memcached and Redis.

                                INTERNET
                               /    |   \
                              /     |    \
                          ENTRY   ENTRY   ENTRY
                          POINT   POINT   POINT
                                    |
                         LOAD BALANCER WEB CACHE
                                    |
                    ---------------------------------
                    |          |                    |
                   WEB        WEB       . . .      WEB
                  CACHE      CACHE                CACHE
                               |
                         LOAD BALANCER WEB SERVICE
                                    |
                    ---------------------------------
                    |         |                     |
                   WEB       WEB       . . .       WEB
                 SERVICE   SERVICE               SERVICE
                    /\        /\                    /\
              /----/--\------/--\------------------/--\
             /         \         \                     \
            DATA        \---------\---------------------\
            BASE                       \
                              LOAD BALANCER DB CACHE
                                        |
                        ---------------------------------
                        |          |                    |
                       DB         DB       . . .       DB
                      CACHE      CACHE                CACHE

Monitoring and alerting             In order to detect and correct errors before end users are affected


Orchestration   The automated configuration and coordination of complex IT systems and services
    - handling a bunch of different nodes with different workloads
    - managing the complexity of deploying a hybrid setup
    - modifying deployments across several Data centers

Infrastructure as Code      uses special machine-readable config files to automate configuration management

Most Cloud providers offer their own tool for managing resources as code:
    - Amazon:    CloudFormation
    - Google:    Cloud Deployment Manager
    - Microsoft: Azure Resource Manager
    - OpenStack: Heat Orchestration Templates

Terraform 
    - Tool for Orchestration
    - uses Domain Specific Language
    - uses each Provides API -> use one tool for every Cloud provider
    
Puppet
    - also ships with a bunch of plug-ins that can be used to interact with the different Cloud providers to 
      create and modify the desired Cloud infrastructure

Nodes in Cloud Environment
    - long-lived        will be updated regularly, e.g. internal mail servers, document servers, ...
    - short-lived       will be deleted and replaced by new ones

------------------------------------------------------------------------------------------------------------------
Building Software in the Cloud

Storage Solutions:
    - Block Storage (traditional) - same as harddrive to a normal PC
    - Object/ Blob Storage        - lets you place and retrieve objects in a storage bucket (e.g. binary cat videos, photos, ...)
                                    storage buckets are addressed by name via API
                                    Blob: Binary Large OBject
                                    Blobs are pieces of data that are stored as independent objects, and require no file system.

    - Database as a Service
            -> SQL   (relational database)
            -> NoSQL very scalable, ultra-fast; but can only be used via DB-API

Storage Class:
    - Performance
    - Availability
    - how often is the data accessed?
    - Throughput, IOPS, Latency are price-relevant
        Throughput    > The amount of data that you can read and write in a given amount of time
        IOPS          > Input/Output Operations Per Second (IOPS)
                        Measures how many reads or writes you can do in one second, no matter how much data you are accessing
        Latency       > The amount of time it takes to complete a read or write operation

Time to first byte      time it takes a storage system to start delivering data after a read request has been made

        Hot data      > Accessed frequently and stored in hot storage    --> normally SSD
        Cold data     > Accessed infrequently and stored in cold storage

-> virtual disks:       - easily move the data around,
                        - migrate the information on the disk to a different location, 
                        - attach the same disk image to other machines,
                        - create snapshots of the current state.

-> Persistent Storage:  Used for instances that are long lived and need to keep data across reboots and upgrades

-> Ephemeral Storage :  Used for instances that are only temporary and only need to keep local data while they're running
                        used e.g. in Containers

Shared File System Solutions    needed if data shall be shared; offered as Software as a Service
                                the data can be accessed through network file system protocols like NFS or CIFS. This lets you connect 
                                            many different instances or containers to the same file system


Load-Balancing
    - Round-Robin DNS:  DNS         Domain Name System      url <-> IP address
                        The Round-robin approach serves clients one at a time, starting with the first, and making rounds until it reaches the beginning again.
                        if a url is navigated to the DNS server responds with a pool of urls in turn
                        if one url is not reachable another one is chosen
                        disadvantages:
                                - no influence on which server is chosen
                                - an overloaded server is still in the pool and can be reached to
    
    - Load Balancer:    - own Rules, can be very easy or super complex
                            Sticky sessions:        All requests from the same client always go to the same backend server
                        - perform health checks -> only healthy backends are reached to
                        - add / remove Servers from server pool; Load Balancer supports Autoscaling
                        - use next Load Balancer to a Client via GeoDNS and GeoIP
                            Content Delivery Networks (CDN): Make up a network of physical hosts that are geographically located as close to
                                                                the end users as possible

Change Management
    keep Services running and innovate / update system by fixing bugs and improving features

    Step 1:
                Unit tests
                Integration tests
                Continuous Integration System (CI): Will build and test our code automatically every time there is a change
                CI-Systems: Jenkins, Travis CI (github)
                Continuous Deployment (CD) via a Test & Production Environment
                                                Environment: Everything needed to run the service
                                                --> Test/Prod Env: Automatic Deployment to Test Env; release manually to Prod
                                                --> Dev/Pre-Prod/Prod Env: push all Changes to Test, release specific Changes to Pre-Prod, Manual Push to Prod
                                                --> A/B testing: Some requests are served using one set of code and configuration (A) and other requests are 
                                                                    served using a different set of of code and configuration (B)
                Build aritfacts

Limitations:
    - Quotas
    - Limits
    - Rate Limits:  Prevent one service from overloading the whole system, e.g. 1 call per Second of expensive API call
    - Utilization Limits: Cap the total amount of a certain resource that you can provision (e.g. Autoscaling with a huge peak can result in several 
                                    additional VMs which can be costy - if you run into a defined limit ask cloud provider for a quota increase)
    - Dependencies - from cloud provider

--------------------------------------------------------------------------------------------------------------------------------------------------------
Monitoring and Alerting

Monitoring lets us look into the history and the current status of a system.

Monitoring System:
    - AWS Cloud Watch
    - Google Stack Driver
    - Azure Metrics
    - Prometheus
    - Data Dog
    - Nagios

Whitebox Monitoring:        Checks the behaviour of the system from the inside
                            Define variables that describe the running system
Blackbox Monitoring:        Checks the behaviour of the system from the outside
                            Check received response with expected response

Metrics:
    - generic, e.g. how much memory an instance is using
    - specific to a given service that shall be monitored

Metrics can be:
    - pull model: service pulls metrics from a monitoring system
    - push model: When push monitoring is used, the service being monitored actively sends metrics to the monitoring system.

To interpret Metrics compare them to their history Metrics and compare them to depending other Metrics.

You only want to store the metrics that you care about, since storing all of these metrics in the system takes space, 
        and storage space costs money.

HTTP response status codes:
    - 100-199: Informational responses
    - 200-299: Successful responses
    - 300-399: Redirects
    - 400-499: Client errors
    - 500-599: Server errors

    400 - 499: Error on Client Side / Browser
    500 - 599: Error on Server Side

Alerts
- watch a system periodically and send e-mails when system does not perform as expected
- on Linux systems this can be done with "cron" (tool to schedule periodic jobs)
- Raising an alert signals that something is broken and a human needs to respond

Differentiate Alerts:
    - Immediate Attention --called--> "Pages" (comes from Pager)
    - need Attention in the near Future --called--> Bugs
    - need no Attention --> do not send any alerts

All Alerts should be Actionable (you should receive an information what to do)

No system is ever available 100% of the time, it's just not possible. But depending on how critical the service is, it can have different 
    Service Level objectives (SLO): SLOs are pre-established performance goals for a specific service.
SLOs need to be measurable, which means that there should be metrics that track how the service is performing and let you check if it's 
    meeting the objectives or not.
Many SLOs are expressed as how much time a service will behave as expected. For example, a service might promise to be available 99% of the time.
Availability targets like this one are commonly named by their number of nines. 
    - 99% example would be a two 9 service, 
    - 99.9% availability is a three 9 service, 
    - 99.999% availability is a five 9 service. Five nine services promised a total down time of up to five minutes in a year.

Service Level Agreement (SLA)    is a commitment between a provider and a client.
Service level objectives though are more like a soft target, it's what the maintaining team aims for

Error budget        e.g. three 9 service means the service can be off for 43 minutes per month <- that is the error budget

Goals should be redefined over time making use of the monitoring results (understand the system, monitor it closed and set realistic targets)

-------------------------------------------------------------------------------------------------------------------------------------
Troubleshooting and Debugging

In case a VM is misconfigured to a Version 2 and will not start anymore:
    - Deploy Version 1 of the VM, so it will start again
    - Copy disk image of Version 2 and mount it in another VM to analyse what went wrong

Methods of Troubleshooting:
    - Run a test VM in a test environment:  Testing through software is always our best bet in the cloud.
    - Call the service provider:            Part of the beauty of running services in the Cloud is that you aren't responsible for everything!
                                            Most Cloud providers are happy to provide various levels of support.

Try to isolate failure on the server side:
    - is it a regional problem? Try to run the VM in another region
    - is it a performance problem? Try to run the VM in a bigger machine

Try to isolate failure on client side:
    - perform a rollback and compare monitoring of "new" system with "old" system - Where do they deviate?

If you operate a service that stores any kind of data, it's critical that you implement automatic backups and that 
    you periodically check that those backups are working correctly by performing restores

Have multiple points of redundancy:
Have a secondary service running. If the first service fails, the load balancer or the DNS entries can easily be changed to 
    point to the secondary service. Or have your service running in different regions of the world or two different cloud vendors.

Having a well-documented disaster recovery plan:
Keep documentation of your service up-to-date (e.g. how to start the service).

Having automatic backups:
Having automatic backups makes it easier to restore and recover.

systemctl 
    is a utility for controlling the systemd system and service manager. It comes with a long list of options for different functionality,
    including starting, stopping, restarting, or reloading a daemon.
    sudo systemctl status apache2                   # check the status of the web server i.e apache2
    sudo systemctl start apache2                    # start service
    sudo systemctl restart apache2                  # restart the server
    sudo systemctl --type=service | grep jimmy      # let's check for the availability of any service with the keyword "jimmy".
    sudo systemctl stop jimmytest && sudo systemctl disable jimmytest  # stop and disable this service "jimmytest"

netstat
    To find which processes are listening on which ports, we'll be using the netstat command, which returns network-related information
    sudo netstat -nlp    # using a combination of flags along with the netstat command to check which process is using a particular port